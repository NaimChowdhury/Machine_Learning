---
title: "Statistical Learning and Linear Regression"
author: "Naeem Chowdhury"
date: "2/14/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
```



  1. Reproduce slide 18, using R markdown.
  
  - Let $f(x) = x^3 - 5x^2 + x + 2$. That's the truth. Draw $f(x)$ in range $x \in (0,10)$.
  
```{r}
curve(x^3 - 5*x^2 + x + 2, from = 0, to = 10)
```

  - Make 10 training sets. To make each training set, pick 15 random values of $x$ in the range. Generate 15 responses $f(x) + \epsilon$, with $\epsilon \sim \mathbb{N}(0,2)$. 
  
  
## Produce training data, and resulting function values.
```{r}
vecG <- c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
trainingData <- data.frame(id = vecG, stringsAsFactors=TRUE)
# Creating each traing seet
for (i in (1:10)){
  
  vecFx <- vector() # vector of random variables
  vecX <- vector() # vector of randomly generated x
  randEpsilon <- rnorm(15, 0, 2)
  randX <- runif(15, 0, 10)
  for(j in (1:15)){
    
    
    fx <- ((randX[j])^3) - (5*((randX[j])^2)) + randX[j] + 2 + randEpsilon[j]
    
    vecX[j] <- randX[j]
    vecFx[j] <- fx
  }
  trainingData <- cbind(trainingData, data.frame(name = vecX))
  trainingData <- cbind(trainingData, data.frame(name2 = vecFx))
}
colnames(trainingData) <- c("dummy", "x1", "fx1", "x2", "fx2", "x3", "fx3", "x4", "fx4", "x5", "fx5", "x6", "fx6", "x7", "fx7", "x8", "fx8", "x9", "fx9", "x10", "fx10")
trainingData <- trainingData[,-1]

fun.1 <- function(x) x^3 - 5*x^2 + x + 2

```

```{r}
## Creating Testing Sets
vecG <- c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
testingData <- data.frame(id = vecG, stringsAsFactors=TRUE)
# Creating each testing set
for (i in (1:10)){
  
  vecFx <- vector() # vector of random variables
  vecX <- vector() # vector of randomly generated x
  randEpsilon <- rnorm(15,0,2)
  randX <- runif(15,0,10)
  for(j in (1:15)){
    
    fx <- ((randX[j])^3) - (5*((randX[j])^2)) + randX[j] + 2 + randEpsilon[j]
    
    vecX[j] <- randX[j]
    vecFx[j] <- fx
  }
  testingData <- cbind(testingData, data.frame(name = vecX))
  testingData <- cbind(testingData, data.frame(name2 = vecFx))
}
colnames(testingData) <- c("dummy", "x1", "fx1", "x2", "fx2", "x3", "fx3", "x4", "fx4", "x5", "fx5", "x6", "fx6", "x7", "fx7", "x8", "fx8", "x9", "fx9", "x10", "fx10")
testingData <- testingData[,-1]
```


## Showing a sample plot

```{r}
fun.1 <- function(x) x^3 - 5*x^2 + x + 2

ggplot(data = trainingData, mapping = aes(x = x2, y = fx2)) + 
  geom_point() +
  stat_function(fun = fun.1) + xlim(0,10)
  
```

```{r}
## Creating Models and Calculating MSE for training and test
dummyVec <- c(1,2,3,4,5)
dummyMatrix <- matrix(0, ncol = 10, nrow = 5)
testMSE.df <- data.frame(dummyMatrix)
## Polynomial 1
polyMSEs <- data.frame(id = dummyVec, stringsAsFactors=TRUE)

for (i in (1:10)){ # For each training set


  xName <- paste("x",i, sep="")
  fxName <- paste("fx",i, sep="")
  trainingMSEs <- c()
  
  for (k in (1:5)){
    lm.obj <- lm(trainingData[,(i*2)] ~ poly(trainingData[,i*2-1],k))
  
    
    traingMSE <- mean((trainingData[,(i*2)] - lm.obj$fitted.values)^2)
    trainingMSEs <- append(trainingMSEs, traingMSE)
    
    testMSEs <- c()
    
    
    for (j in (1:10)){
      
      prediction <- as.vector( predict(lm.obj, as.data.frame( testingData[, (2*j-1)]  ) ) )
      testMSE <- mean(( testingData[,(2*j)] - prediction)^2)
      testMSEs <- append(testMSEs, testMSE)
      
    }
    
    avgtestMSE <- mean(testMSEs)
    
    testMSE.df[k,i] <- avgtestMSE
  }
  
  polyMSEs <- cbind(polyMSEs, trainingMSEs)

}

colnames(polyMSEs) <- c('degree','trainMSE1','trainMSE2','trainMSE3','trainMSE4','trainMSE5','trainMSE6','trainMSE7','trainMSE8','trainMSE9','trainMSE10')

polyMSEs <- polyMSEs %>% mutate(avgMSE = Reduce("+",.)/length(.))


colnames(testMSE.df) <- c('testMSE1','testMSE2','testMSE3','testMSE4','testMSE5','testMSE6','testMSE7','testMSE8','testMSE9','testMSE10')

testMSE.df <- testMSE.df %>% mutate(avg = Reduce("+",.)/length(.))

```

## Problem 3

```{r}
library(ISLR)
View(Auto)
```

a. Produce a scatterplot matrix which includes all of the variables in the data set
```{r}
pairs(Auto)
```

b. Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the "name" variable, which is qualitative.
```{r}
cor(Auto[,names(Auto) !="name"])
```

c. Perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results.
```{r}
regression.model = lm(mpg ~. -name, data = Auto)
summary(regression.model)
```

  -Is there a relationship between the predictors and the response?
  
  The predictors have a nonzero response with the relationship, but some of the relationships are not statistically significant. The adjusted $R^2$ value of 0.82 tell us that about 82% of the variances in _mpg_ are explained by the predictors. 
  
  -Which predictors appear to have a statistically significant relationship to the response?
  
  We can say that the predictors with a $p$-value of less than 0.05 (under significance level of 95%) have a statistically significant relationship to the response. (displacement, origin, year, weight)
  
  -What does the coefficient for the year variable suggest?
  
  A coefficient of 0.75 suggests that the average effect of the passing of a year is 0.75 increase in _mpg_ when all other predictors are fixed.
  
  
d. Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plots identify any observations with unusually high leverages?

  In the first graph, we can see that the relationship between the response and the predictors is non-linear. The residuals seem normally distributed and right-skewed. The leverage graph shows no high leverage points except for point 14.

```{r}

par(mfrow=c(2,2))
plot(regression.model)

```

e. Use the * and : symbols to fit linear regression models with interaction effects.

```{r}
regression.model = lm(mpg ~.-name-cylinders-acceleration+year:origin+displacement:weight+
                  displacement:weight+acceleration:horsepower+acceleration:weight, data=Auto)
summary(regression.model)
```

displacement, origin, weight, and year are used due to their statistical significance. The summary confirms.

f. Try a few different transformations of the variables. Comment on the results.

```{r}
# Model with horsepower interacting with weight.
lm.fit = lm(mpg ~ origin+weight+year+displacement+(horsepower*weight), data=Auto)
summary(lm.fit)
```
All of the coefficients are statistically significant except for displacement, with a higher $p$-value of 0.18. Horsepower is now significant when interacting with weight.

```{r}
# linear combination of logs
lm.fit = lm(mpg ~ log(cylinders)+log(origin)+log(weight)+log(year)+log(displacement)+log(acceleration), data=Auto)
summary(lm.fit)
```

In this model, only origin, weight, and year are significant. 


  
  